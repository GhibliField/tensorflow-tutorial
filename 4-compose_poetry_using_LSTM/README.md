## 简介
基于 TensoFlow 构建两层的 RNN，采用 4 万多首唐诗作为训练数据，实现可以写古诗的 AI demo。

## 步骤简介

- generate_poetry.py - 古诗清洗、过滤较长或较短古诗、过滤即非五言也非七言古诗、为每个字生成唯一的数字ID、每首古诗用数字ID表示；
- poetry_model.py - 两层RNN网络模型，采用LSTM模型；
- train_poetry.py - 训练LSTM模型；
- predict_poetry.py - 生成古诗，随机取一个汉字，根据该汉字生成一首古诗。


## 获取训练数据

**chat.conv **:4 万首古诗数据

## 数据预处理
使用**generate_poetry.py**
处理思路：

数据中的每首唐诗以 [ 开头、] 结尾，后续生成古诗时，根据 [ 随机取一个字，根据 ] 判断是否结束。
两种词袋：“汉字 => 数字”、“数字 => 汉字”，根据第一个词袋将每首古诗转化为数字表示。
诗歌的生成是根据上一个汉字生成下一个汉字，所以 x_batch 和 y_batch 的 shape 是相同的，y_batch 是 x_batch 中每一位向前循环移动一位。前面介绍每首唐诗 [开头、] 结尾，在这里也体现出好处，] 下一个一定是 [（即一首诗结束下一首诗开始）
具体可以看下面例子：

	x_batch：['[', 12, 23, 34, 45, 56, 67, 78, ']']
	y_batch：[12, 23, 34, 45, 56, 67, 78, ']', '[']

下面我们可以看下预处理后的数据长啥样，可以在终端中一步一步执行下面命令：

启动 python：

	python
构建数据：

	from generate_poetry import Poetry
	p = Poetry()
查看第一首唐诗数字表示：

	print(p.poetry_vectors[0])

输出：[1, 1101, 5413, 3437, 1416, 555, 5932, 1965, 5029, 5798, 889, 1357, 3, 397, 5567, 5576, 1285, 2143, 5932, 1985, 5449, 5332, 4092, 2198, 3, 3314, 2102, 5483, 1940, 3475, 5932, 3750, 2467, 3863, 1913, 4110, 3, 4081, 3081, 397, 5432, 542, 5932, 3737, 2157, 1254, 4205, 2082, 3, 2]

根据 ID 查看对应的汉字（[查看输出]）：

	print(p.id_to_word[1101])

输出：寒
根据汉字查看对应的数字（[查看输出]）：

	print(p.word_to_id[u"寒"])

输出：1101
查看 x_batch、y_batch（[查看输出]）：

	x_batch, y_batch = p.next_batch(1)
	x_batch
	y_batch

x_batch [ 1, 1101, 5413, 3437, 1416, 555, 5932, 1965, 5029, 5798, 889, 1357, 3, 397, 5567, 5576, 1285, 2143, 5932, 1985, 5449, 5332, 4092, 2198, 3, 3314, 2102, 5483, 1940, 3475, 5932, 3750, 2467, 3863, 1913, 4110, 3, 4081, 3081, 397, 5432, 542, 5932, 3737, 2157, 1254, 4205, 2082, 3, 2]

y_batch [1101, 5413, 3437, 1416, 555, 5932, 1965, 5029, 5798, 889, 1357, 3, 397, 5567, 5576, 1285, 2143, 5932, 1985, 5449, 5332, 4092, 2198, 3, 3314, 2102, 5483, 1940, 3475, 5932, 3750, 2467, 3863, 1913, 4110, 3, 4081, 3081, 397, 5432, 542, 5932, 3737, 2157, 1254, 4205, 2082, 3, 2, 1]



## LSTM 模型

上面我们将每个字用一个数字表示，但在模型训练过程中，需要对每个字进行向量化，Embedding 的作用按照 inputs 顺序返回 embedding 中的对应行，类似：

	import numpy as np
	embedding = np.random.random([100, 10])
	inputs = np.array([7, 17, 27, 37])
	print(embedding[inputs])


 使用**poetry_model.py**定义网络结构.

## 训练 LSTM 模型

使用**train_poetry.py**.每批次采用 50 首唐诗训练，训练 40000 次后，损失函数基本保持不变，GPU 大概需要 2 个小时左右。当然你可以调整循环次数，节省训练时间，亦或者直接下载训练好的模型(unzip poetry_model.zip)。

##生成古诗

使用**predict_poetry.py**. 根据 [ 随机取一个汉字，作为生成古诗的第一个字，遇到 ] 结束生成古诗。*每次执行生成的古诗不一样*
